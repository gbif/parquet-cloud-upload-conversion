package org.gbif.hadoop.parquet;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.RemoteIterator;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.ParquetInputFormat;
import org.apache.parquet.hadoop.ParquetOutputFormat;
import org.apache.parquet.hadoop.api.DelegatingReadSupport;
import org.apache.parquet.hadoop.api.DelegatingWriteSupport;
import org.apache.parquet.hadoop.api.InitContext;
import org.apache.parquet.hadoop.example.ExampleInputFormat;
import org.apache.parquet.hadoop.example.ExampleOutputFormat;
import org.apache.parquet.hadoop.example.GroupReadSupport;
import org.apache.parquet.hadoop.example.GroupWriteSupport;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.apache.parquet.hadoop.metadata.ParquetMetadata;
import org.apache.parquet.schema.MessageType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.apache.parquet.hadoop.example.GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA;

/**
 * Converts the older Parquet format generated by Hive into the newer format accepted by Google BigQuery,
 * and also sent to MS Azure and AWS Public Data.
 */
public class ParquetUpgrader extends Configured implements Tool {

  private static final Logger LOG = LoggerFactory.getLogger(ParquetUpgrader.class);

  public static final class MyReadSupport extends DelegatingReadSupport<Group> {
    public MyReadSupport() {
      super(new GroupReadSupport());
    }

    @Override
    public org.apache.parquet.hadoop.api.ReadSupport.ReadContext init(InitContext context) {
      return super.init(context);
    }
  }

  public static final class MyWriteSupport extends DelegatingWriteSupport<Group> {
    public MyWriteSupport() {
      super(new GroupWriteSupport());
    }

    @Override
    public WriteContext init(Configuration configuration) {
      return super.init(configuration);
    }
  }

  @Override
  public int run(String[] args) throws Exception {
    LOG.info("----------------------------------------------------------------------------------");
    LOG.info("Starting Parquet schema update with arguments {}", args);
    LOG.info("----------------------------------------------------------------------------------");

    GenericOptionsParser optionParser = new GenericOptionsParser(getConf(), args);
    String[] remainingArgs = optionParser.getRemainingArgs();
    if ((remainingArgs.length != 3)) {
      System.err.println("Usage: parquetupgrader <in> <out>");
      System.exit(2);
    }
    Path inputFile = new Path(args[1]);
    Path outputFile = new Path(args[2]);

    // Read schema from one of the source files
    Path parquetFilePath = null;
    RemoteIterator<LocatedFileStatus> it = FileSystem.get(getConf()).listFiles(inputFile, true);
    while (it.hasNext()) {
      FileStatus fs = it.next();
      if (fs.isFile()) {
        parquetFilePath = fs.getPath();
        break;
      }
    }
    if (parquetFilePath == null) {
      LOG.error("No file found for {}", inputFile);
      return 1;
    }

    // Handle schema
    LOG.info("Getting schema from " + parquetFilePath);
    ParquetMetadata readFooter = ParquetFileReader.readFooter(getConf(), parquetFilePath);
    MessageType schema = readFooter.getFileMetaData().getSchema();
    LOG.info("Read schema: {}", schema);
    String newSchema = ParquetCloudUploadConverter.updateSchema(schema);
    LOG.info("Replacement schema: {}", schema);
    getConf().set(PARQUET_EXAMPLE_SCHEMA, newSchema);

    Job job = Job.getInstance(getConf(), "Parquet upgrade of "+outputFile.getName());
    job.setJarByClass(ParquetUpgrader.class);
    job.setMapperClass(ParquetCloudUploadConverter.class);
    job.setNumReduceTasks(0);
    //job.setOutputKeyClass(LongWritable.class);
    //job.setOutputValueClass(Group.class);
    job.setInputFormatClass(ExampleInputFormat.class);
    job.setOutputFormatClass(ExampleOutputFormat.class);
    // Increase memory as the monthly downloads are pretty large.
    job.getConfiguration().set("mapreduce.map.memory.mb", Integer.toString(16 * 1024));

    ParquetInputFormat.addInputPath(job, inputFile);
    ParquetInputFormat.setReadSupportClass(job, MyReadSupport.class);

    ParquetOutputFormat.setOutputPath(job, outputFile);
    ParquetOutputFormat.setWriteSupportClass(job, MyWriteSupport.class);

    CompressionCodecName codec = CompressionCodecName.SNAPPY;
  	LOG.info("Output compression: " + codec);
    ParquetOutputFormat.setCompression(job, codec);

    job.waitForCompletion(true);

    // Tidy up output files
    LOG.info("Tidying up resulting files");
    it = FileSystem.get(getConf()).listFiles(outputFile, true);
    while (it.hasNext()) {
      FileStatus fs = it.next();
      if (fs.isFile()) {
        Path from = fs.getPath();
        if (from.getName().endsWith("parquet")) {
          Path to = Path.mergePaths(from.getParent(), new Path("/" + from.getName().replace("part-m-", "0").replace(".snappy.parquet", "")));
          FileSystem.get(getConf()).rename(from, to);
          LOG.info("{} renamed to {}", from, to);
        } else {
          FileSystem.get(getConf()).delete(from, false);
          LOG.info("{} deleted", from);
        }
      }
    }

    LOG.info("Wrote to {}", outputFile);

    return 0;
  }

  public static void main(String[] args) {
    try {
      int res = ToolRunner.run(new Configuration(), new ParquetUpgrader(), args);
      System.exit(res);
    } catch (Exception e) {
      e.printStackTrace();
      System.exit(255);
    }
  }
}
